\documentclass[12pt,letterpaper,doublespace, oneside]{article}
%\documentclass[12pt]{extarticle}  % Use 14pt globally




%Here are the various packages I use. Some may be duplicated. 
\usepackage{enumerate}
\usepackage{etoolbox}
\usepackage{amsmath,amsthm,amssymb} % math package
\usepackage{mathtools} %to beef up the above package, more math!
\usepackage{tikz} %for drawing 
\usepackage{graphicx} %for including graphics
\usepackage{fancybox} %for some nice formatting options
\usepackage[hidelinks]{hyperref} %for referencing
%hidelinks removes red and green boxes
\usepackage{varwidth} %for some nice width control
\usepackage{mdframed} %for framed environments
\usepackage{mathrsfs} %more math fonts
\usepackage{xcolor} %color package
\usepackage{setspace}
\usepackage{multirow,array}
\usepackage{caption}
\usepackage[utf8]{inputenc}
\usepackage{pdfpages}
\usepackage[numbers, square]{natbib}
\usepackage{titlecaps}
%\usepackage[paper=a3paper]{geometry}
\usepackage{tabularx}
\usepackage{cleveref}
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\usepackage{xstring}
\usepackage{nameref}
\usepackage{amsthm}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{float}
\usepackage[normalem]{ulem}
\usepackage{booktabs} %include in preamble
%Here are the various packages I use. Some may be duplicated. 






%Define colors and symbols%
%\usepackage[notes,backend=biber]{biblatex-chicago}
%\usepackage[authordate-trad,backend=biber]{biblatex-chicago}
\MakeOuterQuote{"}
%some colours
\definecolor{firebrick}{RGB}{178,34,34}
\definecolor{teal}{RGB}{0,128,128}
\definecolor{indigo}{RGB}{75,0,130}
\definecolor{darkblue}{rgb}{0.0,0.0,.7}
\definecolor{darkred}{rgb}{0.6,0.0,0.0}
\definecolor{lightgrey}{RGB}{220, 220, 220}
\definecolor{darkgrey}{HTML}{878787}
\definecolor{forest}{HTML}{004a2f}
\definecolor{dirt}{HTML}{5d4728}
\definecolor{newblue}{HTML}{004fd9}
\definecolor{paleyellow}{HTML}{FFFFD3}
\renewcommand{\thesection}{}  % Remove numbering from \section
\renewcommand{\thesubsection}{}  % Remove numbering from \subsection
\renewcommand{\thesubsubsection}{} 
\DeclareMathAlphabet{\mathbx}{U}{BOONDOX-ds}{m}{n}
\DeclareMathOperator*{\E}{\mathbb{E}}
\SetMathAlphabet{\mathbx}{bold}{U}{BOONDOX-ds}{b}{n}
\DeclareMathAlphabet{\mathbbx} {U}{BOONDOX-ds}{b}{n}
\doublespacing
%\usepackageA{hyphenat}
\DeclareCaptionLabelFormat{blank}{}
\let\cleardoublepage\relax
%Define colors and symbols%








\begin{document}

\begin{titlepage}
    \centering
    \vspace*{\fill}

    \textsc{\Huge The Causal Impact of anti-Depressants at the Individual Level}\\[2em]

	\textsc{\Large Noah Dixon}\\[2em]
	
    %{\Large Noah Dixon}\\[3em]

	\textbf{\textsc{\LARGE {\color{darkred}9-4-24} }}
	
	\vspace*{\fill}

\end{titlepage}

%\title
%\name
%\data
%\maketitle
%\thispagestyle{empty}
\newpage
\UseRawInputEncoding

%defining Chicago as purely capitalized
\newcommand{\capitalizeTitle}[1]{%
    \StrSubstitute{#1}{ }{~}[\title]%
    \expandafter\capitalizetitle\expandafter{\title}%
}

\newcommand{\capitalizetitle}[1]{%
    \expandafter\StrSubstitute\expandafter{#1}{~}{ }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ a }{ A }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ an }{ An }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ and }{ And }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ as }{ As }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ at }{ At }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ but }{ But }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ by }{ By }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ for }{ For }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ from }{ From }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ in }{ In }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ into }{ Into }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ near }{ Near }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ of }{ Of }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ on }{ On }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ onto }{ Onto }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ or }{ Or }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ the }{ The }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ to }{ To }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ under }{ Under }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ upon }{ Upon }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ with }{ With }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ within }{ Within }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ without }{ Without }[\Title]%
    \expandafter\StrSubstitute\expandafter{\Title}{ and }{ And }[\Title]%
    \expandafter\MakeUppercase\expandafter{\Title}%
}

\newcommand{\zz}{\mathbx Z}   %blackboard bold Z
\newcommand{\qq}{\mathbx Q}   %blackboard bold Q
\newcommand{\ff}{\mathbx F}   %blackboard bold F
\newcommand{\rr}{\mathbx R}   %blackboard bold R
\newcommand{\nn}{\mathbx N}   %blackboard bold N
\newcommand{\cc}{\mathbx C}   %blackboard bold C
\newcommand{\dd}{\mathsf D}   
\newcommand{\id}{\operatorname{id}} %for identity map
\newcommand{\im}{\operatorname{im}} %for image of a function
\newcommand{\dom}{\operatorname{dom}} %for domain of a function
\newcommand{\abs}[1]{\left\lvert#1\right\rvert} %for absolute value
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} %for norm
\newcommand{\modar}[1]{\operatorname{mod}{#1}} %for modular arithmetic
\newcommand{\set}[1]{\left\{#1\right\}} %for set
\newcommand{\setp}[2]{\left\{#1\ :\ #2\right\}} %for set with a property
\newcommand{\lag}{\mathcal{L}}

\renewcommand\thepage{}

%Re-defined notations
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\phi}{\varphi}
\renewcommand{\emptyset}{\varnothing}
\renewcommand{\geq}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

%----------------------------------------------
%Theorem, Lemma, Example, Definition etc. environments

%By default, the text in these environments are italicised
\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\theoremstyle{proposition}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
%\newtheorem{theorem}{Theorem}
\theoremstyle{lemma}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{corollary}
\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
%\theoremstyle{definition} %makes text non-italicized
\theoremstyle{example}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\theoremstyle{conclusion}
\newtheorem{conclusion}[theorem]{Conclusion}






\section{Research Proposal}
\noindent\textbf{Question:} Is it possible to infer causality, at the individual level, when administering antidepressants for depressive symptoms? 

\noindent\rule{\linewidth}{0.4pt}

\noindent



\noindent\textbf{Main Idea:}
Ideally, to test this hypothesis, we would run a double-blind N-of-1 study with periodic, blocked randomization. However, with antidepressants, this is infeasible, due to biological concerns (e.g., high stabilization and washout periods) and ethical concerns (e.g., suicidal ideation). Fundamentally, this restricts analysis to the aggregate level -- at which point, time-varying heterogeneity is obscured. Consequently, in order to evaluate the individual-level effect, we need to treat the data as being quasi-experimental. Traditionally, this is where the analysis stopped -- there was no plausible way to extract this much data. However, recent technological advancements have altered the bounds of what is permissible. In Brodersen et al. (2015), for instance, they introduce a Bayesian Structural Time Series (BSTS) model to estimate the individual-level counterfactual time series had the intervention not occurred. Additionally, in Robins (1986), he introduces the g-method (and more specifically, g-computation) -- an approach to individual-level causal inference when there is an abundance of longitudinal data (which has since been significantly expanded on). One serves as a methodological framework; the other as a causal framework. The idea is to \text{``}layer'' one on top of the other, to construct an individual-level counterfactual that uses a conglomerate utility measure rather than a single outcome variable, and which can subsequently be used to identify the individual treatment effect (ITE). From here -- and assuming a true RCT is conducted -- it may be possible to reconstruct the ATE, with individual-level heterogeneity. 


\newpage
\section{Basic Framework}

\noindent\textbf{Motivation:} At the individual level, mental health is diagnosed by weekly tracking an individualâ€™s responses to the HAM-D (Hamilton Depression Rating Scale).\footnote{Although recently, new tests -- like the MADRS (Montgomery-Asberg Depression Rating Scale), QIDS-C (Clinician-rated Quick Inventory of Depressive Symptomatology), and PHQ-9 (Patient Health Questionnaire) -- have gained traction, each successive test placing more emphasis on the individual.} At the population level, tests are validated using an RCT, which is subsequently estimated using an MMRM model. However, while such metrics establish average efficacy, they may not translate to \emph{individual} responses. Fundamentally, these measures do not account for interdependencies across time, individual-level shocks, any form of biometric tracking, most forms of measurement error, and only a small subset of adverse effects. Further, the consensus in the literature is that antidepressants are only beneficial, in the long run, for individuals with severe depression (Fournier et al. 2010; Kirsch et al. 2008), with the effect sizes being statistically small (Cipriani et al. 2018) or negligible for those with small-to-mild depression. Around 11-13\% of adults report using antidepressants, but only 2-3\% of individuals report having severe depression (Brody \& Gu 2020; Villarroel \& Terlizzi 2020). Further, Wong, Motulsky, and Eguale (2016) report that over 50\% of individuals may be using antidepressants for something other than depression. \emph{And all of this assumes that an individualâ€™s utility is inextricably linked to their score on the test.} There are many reasons to believe that the individual-level data neglected under an MMRM model is an integral piece of the puzzle, and thus there is a necessity, in the literature and in the market, to introduce rigorous statistical methodology to deduce the causal impact of antidepressants on a \emph{specific} individual. 

\noindent\rule{\linewidth}{0.4pt}

\noindent
\textbf{Experimental Design} 

\emph{Outcomes:} The outcomes tracked are tri-daily (morning, afternoon, evening) \text{``}mood'' scores and weekly HAM-D scores, in conjunction with \text{``}richer'' measures, including: sleep, step count, mood diary, adverse events, and biomarkers. More generally, if the data is saturated enough, we \emph{may} be able to identify the ITE, absent of the ATE.

\noindent

\emph{Methodology:} Hinges on the methodological framework proposed in (Brodersen et al. 2015): 

\begin{itemize}

\item \textbf{No anticipation.} This is hard to justify, in practice. An individual may alter their behavior in anticipation of the medicine. One way to obviate this is to randomize when an individual begins taking the medication. For example, the individual is told that they will begin taking the medication sometime over the course of the month, but they do not know exactly when this will occur. This is something that could reasonably be approved by IRB. 

\item \textbf{Structural stability of the counterfactual model.} Using a Bayesian Structural Time Series (BSTS) model helps make a sufficient case for this. In practice, it is impossible to justify. One pitfall of this model is that we can never fully recover an unbiased estimate. However, we can recover a \emph{plausible estimate}. At the individual-level, this is probably good enough. 

\item \textbf{No concurrent interventions.} If an individual begins taken epileptic medication at the same time, it is impossible to extract the true effect of the antidepressant. However, it is still possible to extract the aggregate effect of the antidepressant and epileptic medication, given that the other assumptions hold. 

\item \textbf{Sufficient pre-intervention data.} This requires collecting a sufficient amount of data, \emph{prior} to treatment. 

\end{itemize}

\emph{Identification:} The BSTS assumptions have nothing to do with identification. In order to identify, we need to layer on a causal framework. More generally, the basic causal assumptions, as stated in HernÃ¡n and Robins (2020), are:

\begin{itemize}

\item \textbf{Well-defined intervention and outcome}. This requires consistent timing, dosage, and measurement each period. 

\item \textbf{Consistency.} Denote $A_{it}$ as the treatment or exposure for person $i$ at time $t$. Define $Y_{it}$ as being the outcome at time $t$. Then, if $A_{it} = a$, $Y_{it} = Y_{it}(a)$. In other words, there is no interference from \emph{other} periods unless modeled. 

\item \textbf{Positivity.} For all relevant histories, $0 < \text{Pr}(A_{it} = 1| H_{it}) < 1$. This is not necessarily guaranteed, under the counterfactual argument. 

\item \textbf{Sequential Exchangeability.}  Define \[H_{it} = \{ C_i; A_{i,1}, ..., A_{i,t-1}; Y_{i1}, ... Y_{i, t-1}; ... L_{i1} ... L_{i,t}\] as being the \emph{history} for a person $i$ at time $t$, where $C_i$ represents baseline covariates, $A_{i, 1:t-1}$ represents past treatments/doses, $Y_{i, 1: t-1}$ represents past outcomes/symptoms, and $L_{is}$ represents time-varying covariates measured directly before the $t$'th decision. Then, we require that $A_{it} \perp\!\!\!\perp (Y_{i,t}^{(0)}, Y_{i,t}^{(1)}) \mid H_{it}$. This is the hardest assumption to justify. 
%In practice, this is a strong assumption, so we use $A_{it} \perp\!\!\!\perp \Big\{\, Y_{is}(\bar{a}_{i,s}) : s \ge t,\ \bar{a}_{i,t-1} \mid H_{it} \}$ instead.

\end{itemize}

%Note that the methodological conditions imposed by the BSTS differ from the causal conditions imposed by trial emulation. One is associated with methodological concerns; the other, with causal identification. Under these combined conditions, the ITE,
%\[
%\text{ITE}_i = \mathbb{E}\left[ Y_{it}(1) - Y_{it}(0) \mid \mathcal{H}_{it} \right]
%\]
%\textbf{has potential to be identified}, in which 
%\[
%\mathcal{H}_{it} = \{ C_i; A_{i,1}, ..., A_{i,t-1}; Y_{i1}, ... Y_{i, t-1}; ... L_{i1} ... L_{i,t-1} \}
%\]
%represents the history, where $C_i$ represents baseline covariates, $A_{i, 1:t-1}$ represents past treatments/doses, $Y_{i, 1: t-1}$ represents past outcomes/symptoms, and $L_{i, 1: t-1}$ represents time-varying covariates measured directly before the $t$'th decision. \textbf{Identification under these assumptions requires a proof, as this is not a standard model setup.} For instance, in Brodersen et al. (2015), they evaluate multiple individuals at the same time and there goal is not causal inference. Any form of causal interpretation also requires that $\mathcal{H}_{t-1}$ is sufficiently accounted for. Once again, the underlying goal is to \emph{infer} some semblance of causal impact, within a reasonable confidence interval -- not deduce an entirely unbiased estimate.



\noindent

\emph{Estimation:} The standard BSTS model, as shown in (Brodersen et al. 2015), is given by a system of equations: 
\[
Y_t = Z_t^\top \alpha_t + \epsilon_t \qquad \text{[Outcome Equation]},
\]
\[
\alpha_{t+1} = T_t \alpha_t + R_t \eta_t \qquad \text{[State Equation]},
\]
where $\epsilon_t \sim \mathcal{N}(0,\sigma_t^2)$ and $n_t \sim \mathcal{N}(0,Q_t)$ are independent of all other unknowns. The first equation is the outcome equation and represents what we are actually controlling for. 

The variables are represented by: 
\begin{itemize}
\item $Y_t$ is a scalar, which represents the PHQ-9 scores. It can also include other measures. If running a joint model, this opens up an entire other can of worms.  

\item $Z_t$ is essentially a control matrix, which includes a dummy variable if treated. For instance, $Z_t = [1,D_t, S_{1,t}, X_{2,t}]'$ represents the case in which a set of seasonal components are included, as well as a set of covariates. 

\item $\alpha_t$ represents the coefficients associated with each portion of the control matrix. For instance, in the previous example, the corollary $\alpha_t = [\mu_t, \tau, \beta_1, \beta_2]$, where $\mu_t$ represents an individual's underlying mental health trajectory absent of treatment and $\tau$ represents the inferred causal impact of the treatment itself. Obviously, the question is: \emph{how do we deal with $\mu_t$.} This is where the state equation comes into play. 

\item $\alpha_{t+1}$ governs how the latent states evolve over time. That is, how does an untreated individual evolve over time. This is essential in modeling the counterfactual. 

\item $T_t$ is a transition matrix, which tells you how the state evolves, deterministically, over time. The degree to which we model trends, in general, is reflected inside this. \emph{It must match the seasonal trends reflected in $\alpha_t$.}

\item $R_t$ determines which states receive noise and how much noise they receive. 

\item $Q_t$ represents the state disturbance covariance matrix, and controls how much the latent components are allowed to evolve over time.

\end{itemize}

To model this more specifically, suppose the outcome equation is given as: 
\[
Y_t = \mu_t + \tau \cdot D_t + X_{t-1}\beta + \gamma_t + \epsilon_t,
\]
where $X_{t-1} = [\text{Sleep}_{t-1}, \text{Exercise}_{t-1}, \text{Average Heart Rate}_{t-1}, \text{Environmental Stressors}_{t-1}$, $\gamma_t = \gamma_t^{(\text{Week})} + \gamma_t^{(\text{Circadian})} + \gamma_t^{(\text{Seasons})} $ represents latent seasonal effects, and $\mu_t = \mu_{t-1} + \eta_t$ represents how a person's depression would naturally evolve over time, absent of medication or seasonal cycles. In fact, the distribution of $\eta_t$ can align with aggregate population data with certain priors. Note that each $\gamma_t^{(\cdot)}$ has its own transition matrix, such that $\gamma_t(T) = -\sum_{j=1}^{T-1}\gamma_t^{(j)}$. Essentially, these work as dummy variables that are included in the outcome equation, but which are estimated with some degree of uncertainty; they influence both the outcome equation and the state equation. $Q_t$ is either  \emph{learned} through posterior sampling, or estimated from the data, typically using MLE. 

\bigskip

\emph{g-methods:} In the process of learning. 


\emph{Utility:} Rather than evaluating each estimated measure, in isolation, a conglomerate measure of utility should be estimated, based on an individual's ranking of each respective metric. As an example, 
\[
U_i = W_A \cdot \text{Sleep} + W_B \cdot \text{PHQ-9} + W_C \cdot \text{Fulfillment},
\]
where $W_A$ represents arbitrary weights such that the $\sum_{j = 1}^JW_J = 1$ and $j$ represents the number of categories. Likely, using machine learning techniques is the best way to approach this. 



\noindent


\noindent\rule{\linewidth}{0.4pt}

\noindent


\noindent\textbf{Economic Importance:} The goal of causal inference is to evaluate the average effect on an arbitrary member of the population. However, one question is whether this metric is always sufficient, in application. Sometimes, the goal may not be generalizability but individualization. This idea -- of integrating individual-level heterogeneity into traditional aggregate models -- is already influencing modern macroeconomic models. It is also at the heart of the traditional field experiment vs. lab experiment argument. There is no reason to believe that large-scale \text{``}individualized econometrics'' is not coming next. 

Additionally, there may be large welfare gains -- and efficiency gains -- from targeted dosing vs. a one-size-fits-all approach, or from a one-size-fits-all approach to corollary subgroups. 

Finally -- and perhaps more importantly -- it may be possible to aggregate these results together to uncover the population effect (obviously, under a different control/placebo experimental design). That is, we may be able to recover an ATE \emph{with individual-level heterogeneity}, which could have intriguing applications. 

\noindent\rule{\linewidth}{0.4pt}

\noindent
\textbf{Data:} The only way to conduct this study is to run some manipulation of an experiment.

\noindent\rule{\linewidth}{0.4pt}

\noindent
\textbf{Sources:} In Zotero.








%The following is what you do if you were trying to attempt this from a biostatistical perspective. Run a double-blind N-of-1 trial (an experiment on a single individual, in which neither the individual receiving treatment nor the prescribing therapist know the level of medication being received, at any point in time) with varying levels of medication (low, medium, and high) on an 8-10 week basis. Proper \text{``}tapering'' phases need to be built in. For individuals who stop taking the medication, information should still be tracked, to properly evaluate the washout phase and the subsequent utility fluctuation. Analysis does not require individual-level covariates (but does require an abundance of pre-treatment values), although certain covariates should be tracked \emph{for the purpose of identifying missing data}, as well as providing a more comprehensive overview of causal effects. Additionally, proper incentives should be built into the experiment itself, in order to justify the MAR assumption. The outcomes tracked are tri-daily (morning, afternoon, evening) \text{``}mood'' scores and weekly PHQ-9 scores, in conjunction with \text{``}richer'' measures such as: sleep, step count, mood diary, adverse events, and biomarkers. The exact blend of covariates -- to the extent to which they are utilized -- is not as important and is not required for identification.





%\newpage
%\bibliographystyle{chicago}
%\bibliography{BEERthesisREAL.bib}
%\addcontentsline{toc}{section}{References}
\end{document}

